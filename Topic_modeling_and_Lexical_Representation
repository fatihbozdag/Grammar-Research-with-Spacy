## First Import Necessary Packages
from bertopic import BERTopic
from bertopic.representation import KeyBERTInspired
from bertopic.representation import MaximalMarginalRelevance
from umap import UMAP
from hdbscan import HDBSCAN
from bertopic.vectorizers import ClassTfidfTransformer
from keybert import KeyBERT
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction import _stop_words
import pandas as pd
import re
import torch
torch.device("mps")
import spacy
import os
nlp = spacy.load('en_core_web_trf',disable=['ner', 'parser'])
spacy.require_gpu()
nlp.max_length = 10000000000000
pattern = r'ICLE\-\w+\-\w+\-\d+\.\d+'
pattern_ = r'[^\w\s]'

### Next step is required if you want ChatGPT to provide represetations for your terms. Also note that you need an OpenAI account.
import os
import openai
import chardet
openai.organization = "your_OPENai_Organization" 
openai.api_key = ("your_OPENai_API_Key")
from bertopic.representation import OpenAI

###Next clean texts and utilize Spacy for lemmatisation task.
df = pd.read_csv("your_data_here") ### Since ICLE corpus is copyrighted I cannot share the full dataset.
df['text_field'] = df['text_field'].apply(lambda x: re.sub(pattern, '', x).replace('\n', ''))
df = df.rename(columns={'Native language': 'Native_Language'})
df['text_field'] = df['text_field'].apply(lambda x: re.sub(pattern, '', x).replace('\n', ''))

###Next step removes stop words and lemmatize whole corpus

df['lemmatized_text'] = df['text_field'].apply(lambda x: ' '.join([token.lemma_ for token in nlp(x) if not token.is_stop]))
docs = df['lemmatized_text']

# Step 1 - /Users/t embeddings
embedding_model = SentenceTransformer('all-MiniLM-L12-v2')

# Step 2 - Reduce dimensionality
umap_model = UMAP(n_neighbors=20, n_components=5, min_dist=0.0, metric='cosine', random_state=123)
### Note that due to scholastics nature of umap method, without defining a random_state, each run would provide different results.
### Hence, for my study, random_state = 123, for those who wish to test and replicate the study with the same data.

# Step 3 - Cluster reduced embeddings
hdbscan_model = HDBSCAN(min_cluster_size=30, metric='euclidean', cluster_selection_method='eom', prediction_data=True)

# Step 4 - Tokenize topics
vectorizer_model = CountVectorizer(stop_words="english")

# Step 5 - Create topic representation
ctfidf_model = ClassTfidfTransformer()

# Step 6 - (Optional) Fine-tune topic representations with 
# a `OpenAI's ChatGPT` model
representation_model = OpenAI()

topic_model = BERTopic(
  embedding_model=embedding_model,          # Step 1 - Extract embeddings
  umap_model=umap_model,                    # Step 2 - Reduce dimensionality
  hdbscan_model=hdbscan_model,              # Step 3 - Cluster reduced embeddings
  ctfidf_model=ctfidf_model,                # Step 5 - Extract topic words
  representation_model=representation_model,# Step 6 - (Optional) Fine-tune topic representations
  calculate_probabilities=True,verbose=True, nr_topics = 'auto', top_n_words = 10, min_topic_size  = 30)
  
topics, probs = topic_model.fit_transform(docs)
topic_info = topic_model.get_topic_info()
topic_info = topic_info[topic_info['Topic'] != -1] ### this removes outliner documents

#Next step is to calculate entropy score
import numpy as np
doc_topic_matrix = np.array(probs)
normalized_doc_topic_matrix = doc_topic_matrix / doc_topic_matrix.sum(axis=1, keepdims=True)
# Calculate topic entropy
topic_entropy = (-normalized_doc_topic_matrix * np.log2(normalized_doc_topic_matrix)).sum(axis=0)
# Create a DataFrame with topic entropy values
entropy_df = pd.DataFrame({'Topic': range(len(topic_entropy)), 'Entropy': topic_entropy})
# Sort the DataFrame by entropy in descending order
sorted_entropy_df = entropy_df.sort_values('Entropy', ascending=False)
sorted_entropy_df

# Merge Topic Freqs with Entropy DataFrame Display the sorted.
topic_info = topic_info.merge(sorted_entropy_df, on='Topic')

### Visualize the top 10 terms in Topic 0 
import seaborn as sns
import matplotlib.pyplot as plt

topic_freq = topic_model.get_topic_freq()
topic_words = [topic_model.get_topic(topic) for topic in topic_freq['Topic']]
# Select the desired topic
desired_topic = 0

# Filter the topic_words list for the desired topic
filtered_topic_words = [topic_model.get_topic(desired_topic)]

# Create a DataFrame with the filtered data
topic_df = pd.DataFrame(filtered_topic_words[0], columns=['Word', 'c-TF-IDF'])

# Create a bar plot to visualize the words and their c-TF-IDF scores
sns.set(style="whitegrid", font = 'Times New Roman')
plt.figure(figsize=(10, 6))
sns.barplot(x='c-TF-IDF', y='Word', data=topic_df, palette='viridis')
plt.title(f'Terms with Higher c-TF-IDF scores for Topic {desired_topic}')
plt.xlabel('c-TF-IDF Score')
plt.ylabel('Terms')
plt.show()


### Visualize all topics with their frequencies and entropy scores

# Set plot style and size
sns.set(style="whitegrid", font = 'Times New Roman')
plt.figure(figsize=(20, 12))

# Create a bar plot for the 'Count' column
bar_plot = sns.barplot(x="CustomName", y="Count", data=topic_info, color="b", alpha=0.6)

# Set the x-axis and y-axis labels with increased font size for the x-axis label
bar_plot.set_xlabel("Custom Term Labels", fontsize=14)
bar_plot.set_ylabel("Frequency")

# Rotate x-axis labels for better readability and increase font size
bar_plot.set_xticklabels(bar_plot.get_xticklabels(), rotation=90, fontsize=12)

# Create a line plot for the 'Entropy' column with a secondary y-axis
entropy_plot = bar_plot.twinx()
sns.lineplot(x="CustomName", y="Entropy", data=topic_info, color="r", marker="o", linestyle="-", ax=entropy_plot)

# Set the y-axis label for the line plot
entropy_plot.set_ylabel("Entropy Scores")

# Find the label with the highest entropy
highest_entropy_label = topic_info.loc[topic_info['Entropy'].idxmax()]

# Annotate the label with the highest entropy
bar_plot.annotate('Highest entropy', xy=(highest_entropy_label["CustomName"], highest_entropy_label["Count"]),
                  xytext=(highest_entropy_label["CustomName"], highest_entropy_label["Count"]+0.05*topic_info["Count"].max()),
                  arrowprops=dict(facecolor='black', shrink=0.05),
                  fontsize=12, color='black', rotation=90)

# Set the title for the plot


# Show the plot
plt.show()
### The end of Topic Modeling. Next is to train Gensin Word2Vec model over the essays sharing Topic 0 ###

from pprint import pprint
from sklearn.neighbors import KNeighborsRegressor
from sklearn.feature_extraction import _stop_words
from gensim.models import word2vec
from gensim.utils import simple_preprocess
from gensim.utils import simple_preprocess
from gensim.models import Word2Vec

documents = pd.read_csv('your_topic_modeling_results_file')
documents.loc[documents['Topic']== 0, 'Topic'] = 'Common_Topic'
documents.loc[documents['Topic']!= 'Common_Topic', 'Topic'] = 'Other_Topics'
topic_counts = documents['Topic'].groupby(documents['Native_Language']).value_counts(sort = True)

### setseed ###
def hash(astring):
  return ord(astring[71]) 
  
  ### A note of caution! Gensim results are randomized, so it is necessary to set seed.###
  ###However, as I was not aware that 'workers' parameter should have been set to 1 for non-randomness, the results may slightly vary###
  ### Our results are as follows; 
Related words for 'academic':
background: 0.7666
achieve: 0.7601
professional: 0.7572
market: 0.7565
unemployment: 0.7442
argue: 0.7357
qualification: 0.7139
demand: 0.7091
general: 0.6999
base: 0.6768

Related words for 'education':
continue: 0.8206
public: 0.8113
exist: 0.7899
rich: 0.7625
poor: 0.7441
private: 0.7397
equal: 0.7304
rate: 0.7285
afford: 0.7230
receive: 0.7221

Related words for 'university':
unfortunately: 0.7310
degree: 0.7270
graduation: 0.7104
field: 0.7103
major: 0.6973
useless: 0.6969
value: 0.6932
preparation: 0.6568
reality: 0.6360
employment: 0.6353

Related words for 'graduate':
qualification: 0.8626
employer: 0.8495
employ: 0.8368
graduation: 0.8299
employment: 0.8297
employee: 0.8124
unemployment: 0.8099
qualified: 0.8053
position: 0.7758
market: 0.7729

Related words for 'college':
succeed: 0.8101
choice: 0.6463
right: 0.6356
success: 0.6338
enter: 0.6049
attend: 0.5974
employment: 0.5855
qualified: 0.5627
graduate: 0.5260
level: 0.5200 ####
  
  
### Train model ###

def train_word2vec(common_topics, lemmatized_text_column, target_words):
    # Preprocess the text data in the lemmatized_text column
    common_topics["preprocessed_text"] = common_topics['lemmatized_text'].apply(lambda x: simple_preprocess(x))

    # Train the Word2Vec model on the preprocessed text
    model = Word2Vec(common_topics["preprocessed_text"].tolist(), vector_size=500, window=10, min_count=100,
                     workers=10, sg = 0, hs = 1, negative = 20, ns_exponent = 0.75, epochs = 50, sample=1e-5,alpha=0.03,hashfxn=hash)

    related_words_dict = {}

    # Find related words for each target word
    for word in target_words:
        if word in model.wv.key_to_index:
            related_words = model.wv.most_similar(word, topn=10)
            related_words_dict[word] = related_words
        else:
            related_words_dict[word] = None

    return related_words_dict

# Assuming you already have a dataframe called common_topics with a column 'lemmatized_text'
# If not, you can create a sample dataframe as follows:
# common_topics = pd.DataFrame({"lemmatized_text": ["text1", "text2", "text3"]})

# Define the target words
target_words = ["academic", "education", "university", "graduate", "college"]

# Call the function
related_words = train_word2vec(common_topics, "lemmatized_text", target_words)

# Print the related words for each target word
for word, related in related_words.items():
    if related:
        print(f"\nRelated words for '{word}':")
        for related_word, similarity in related:
            print(f"{related_word}: {similarity:.4f}")
    else:
        print(f"\n'{word}' not found in the vocabulary.")

### Visualize Results in a Semantic Map ###

def plot_semantic_map_connected(related_words_dict, threshold=0.5):
    G = nx.Graph()

    # Add target words as nodes
    for word in related_words_dict.keys():
        G.add_node(word, color="red")

    # Add related words as nodes and edges with weights based on similarity
    for word, related in related_words_dict.items():
        for related_word, similarity in related:
            # If related_word is not in the graph, add it with color blue
            if not G.has_node(related_word):  
                G.add_node(related_word, color="lightblue")
            # If related_word is in the graph but its color is not yellow (it's not a target word), 
            # change its color to blue
            elif G.nodes[related_word]["color"] != "red":
                G.nodes[related_word]["color"] = "lightblue"
            G.add_edge(word, related_word, weight=similarity)
    
    # Set up the plot
    plt.figure(figsize=(25, 15))

    # Compute node positions using the Kamada-Kawai layout algorithm
    pos = nx.kamada_kawai_layout(G, scale=2.5, )

    # Set node sizes and colors
    node_sizes = [3000 if node[1]["color"] == "red" else 1000 for node in G.nodes(data=True)]
    node_colors = [node[1]["color"] for node in G.nodes(data=True)]  # directly use node color

    # Draw nodes and labels
    nx.draw_networkx_nodes(G, pos, node_color=node_colors, alpha=0.6, node_size=node_sizes)
    nx.draw_networkx_labels(G, pos, font_size=14, font_weight="bold", font_family="sans-serif")

    # Draw edges with varying color based on the similarity score
    for (u, v, d) in G.edges(data=True):
        if d['weight'] > 0.7:
            edge_color = 'black'
        elif d['weight'] > 0.5:
            edge_color = 'green'
        else:
            edge_color = 'red'
        nx.draw_networkx_edges(G, pos, edgelist=[(u, v)], width=d["weight"] * 2, edge_color=edge_color, style = 'dashed')

    # Add edge labels with weight scores rounded to 2 decimal places
    edge_labels = {(u, v): '{:.2f}'.format(d["weight"]) for u, v, d in G.edges(data=True)}
    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color='red')

    # Display the plot
    plt.axis("off")
    plt.show()

# Call the function to plot the connected semantic map
plot_semantic_map_connected(related_words, threshold=0.9)

